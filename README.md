# jsonintodb
use python to push data into database; bulk insertion

(개요)
대용량 json을 parsing하는것은 시간적으로 큰 무리가 있기때문에,
데이터베이스에 넣어서 속도를 빠르게 합니다.
초기 약 850mb의 json을 jq를 통해 250mb 정도로 축소하고,
필요한 데이터만 pandas dataframe을 이용해 db에 insert 해줍니다.

(string concatenation에 +=를 사용한 이유)
string을 join하는데에는 +를 이용하여 join하는 방법과
.join() 메서드를 이용하여 join하는 방법 둘 다 시도해보았으나,
join 메서드를 사용하는것과 +를 사용하는 것 둘 다 큰 시간 차이가 없었고,
오히려 join 메서드가 직관적으로 알아보기 힘들다는 단점이 있어 결국 +를 채택하였습니다.

(소요 시간)
초기 방식은 한 행마다 insert를 하는 방법을 사용하였더니
약 20만 row를 삽입하는데 6시간이 걸렸고, 이를 개선하여 이중 루프가 아닌 바깥 루프에서만 insert 하도록 개선하여
v1 기준 소요시간은 약 500만 row 기준 1:13:41.558586 만큼 걸렸습니다(프로세서에 따라 다를 수 있습니다)
루프 바깥에서 한꺼번에 insert 하는 방법도 시도해보았으나, 쿼리 데이터 한계를 1GB로 늘렸음에도 불구하고
서버 사양이 좋지 못하여 데이터베이스가 재부팅되는 불상사가 발생하였기에,
바깥 루프를 한번 돌때마다 insert 하는 방식으로 수정하였고, 문제없이 데이터베이스에 값을 넣을 수 있었습니다.
